{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b40a23a-7576-4bb3-b673-5dc5cc3466e0",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faea5188-a005-4e77-a147-9091c4f0e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lokeshdash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lokeshdash/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 125731\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# load data\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Total Tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632e720-f208-4b28-8f63-66ede947ea42",
   "metadata": {},
   "source": [
    "#### Here, we converted the text to lowercase (to maintain consistency) and used word_tokenize to break the entire corpus into word-level tokens. This prepares our data for model training by converting raw text into a structured format that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46dba4-2ff0-4d42-8dca-7646a9e8f42d",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d74b86c7-d945-4fc7-8da1-8b72b8713043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(tokens)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab = sorted(set(tokens + [\"<UNK>\"]))  # ensure <UNK> is added\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed627c-c78c-4ea9-8cec-5e35fab5f069",
   "metadata": {},
   "source": [
    "#### Here, we counted how often each word appears using Counter, then sorted the vocabulary from most to least frequent. This sorted list helps us assign lower indices to more common words (useful for embeddings). Then, we created word2idx and idx2word dictionaries to convert words to unique IDs and back. Finally, we stored the total vocabulary size, which will define the input and output dimensions for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9583da3-bfa3-46fb-919b-140cb469abc7",
   "metadata": {},
   "source": [
    "## Building Input-Output Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c94cc45e-e596-4cfc-b742-d58ad15c54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # ← Add this line\n",
    "\n",
    "sequence_length = 4\n",
    "data = []\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    input_seq = tokens[i:i + sequence_length - 1]\n",
    "    target = tokens[i + sequence_length - 1]\n",
    "    data.append((input_seq, target))\n",
    "\n",
    "def encode(seq):\n",
    "    return [word2idx[word] for word in seq]\n",
    "\n",
    "encoded_data = [(torch.tensor(encode(inp)), torch.tensor(word2idx[target])) for inp, target in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d8fb1-a6ae-4137-b915-d2fab6e46079",
   "metadata": {},
   "source": [
    "#### Here, we used a sliding window approach to generate training samples: for every group of 3 consecutive words (input), we predict the next word (target). It prepares the data for sequence modelling.\n",
    "#### Then, we defined an encode function to convert each word in the sequence into its corresponding index using our vocabulary. Finally, we build encoded_data, a list of (input_tensor, target_tensor) pairs, where each input is a tensor of word indices and the target is the index of the next word to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358edc7d-fe0d-4c39-91f5-6cbc7acb8ae6",
   "metadata": {},
   "source": [
    "## Designing the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebf5377-53f9-4e74-885d-16df54e98897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PredictiveKeyboard(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):\n",
    "        super(PredictiveKeyboard, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])  # last LSTM output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd269be1-df8b-4b3f-ac52-7264d4e36a04",
   "metadata": {},
   "source": [
    "#### This class defines our neural network model. First, the Embedding layer converts word indices into dense vectors. These embeddings are then passed through an LSTM layer, which captures the sequential context of the input.\n",
    "\n",
    "#### Finally, we take the output of the last time step and feed it through a Linear layer to get a vector of size vocab_size, representing the predicted probabilities for each word in the vocabulary. This architecture allows the model to learn patterns and dependencies in word sequences for next-word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d616d1-833c-4440-bfe5-e2ba7eab14de",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad8adcb9-aa3c-4834-bf38-0b85ccc09245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.6774\n",
      "Epoch 2, Loss: 2.1599\n",
      "Epoch 3, Loss: 1.0939\n",
      "Epoch 4, Loss: 0.5619\n",
      "Epoch 5, Loss: 0.1868\n",
      "Epoch 6, Loss: 0.0670\n",
      "Epoch 7, Loss: 0.0304\n",
      "Epoch 8, Loss: 0.0176\n",
      "Epoch 9, Loss: 0.0093\n",
      "Epoch 10, Loss: 0.0053\n",
      "Epoch 11, Loss: 0.0033\n",
      "Epoch 12, Loss: 0.0021\n",
      "Epoch 13, Loss: 0.0016\n",
      "Epoch 14, Loss: 0.0013\n",
      "Epoch 15, Loss: 0.0010\n",
      "Epoch 16, Loss: 0.0009\n",
      "Epoch 17, Loss: 0.0008\n",
      "Epoch 18, Loss: 0.0007\n",
      "Epoch 19, Loss: 0.0006\n",
      "Epoch 20, Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "model = PredictiveKeyboard(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    random.shuffle(encoded_data)\n",
    "    for input_seq, target in encoded_data[:10000]:  # Limit data for speed\n",
    "        input_seq = input_seq.unsqueeze(0)\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target.unsqueeze(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a79232-8442-47f5-9164-5723308acba8",
   "metadata": {},
   "source": [
    "#### Here, we instantiated the model, defined a loss function (CrossEntropyLoss), and used the Adam optimizer for efficient gradient updates. During each training epoch, we shuffled the dataset for better generalization. For each training sample, we added a batch dimension to the input, computed the output, and calculated the loss between predicted and actual next-word indices.\n",
    "\n",
    "#### Then we performed backpropagation, updated the weights, and accumulated the total loss for tracking. This loop trains the model to predict the next word based on the previous sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672b8bc-bb17-4da6-85a6-02e5f2fe8c18",
   "metadata": {},
   "source": [
    "## Predicting the Next Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b683307-4ced-453a-b8cb-a3221ecd54c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions: ['this', 'predict', 'going']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def encode(seq):\n",
    "    return [word2idx.get(word, word2idx[\"<UNK>\"]) for word in seq]\n",
    "\n",
    "def suggest_next_words(model, text_prompt, top_k=3):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(text_prompt.lower())\n",
    "    if len(tokens) < sequence_length - 1:\n",
    "        raise ValueError(f\"Input should be at least {sequence_length - 1} words long.\")\n",
    "\n",
    "    input_seq = tokens[-(sequence_length - 1):]\n",
    "    input_tensor = torch.tensor(encode(input_seq)).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = F.softmax(output, dim=1).squeeze()\n",
    "        top_indices = torch.topk(probs, top_k).indices.tolist()\n",
    "\n",
    "    return [idx2word[idx] for idx in top_indices]\n",
    "\n",
    "print(\"Suggestions:\", suggest_next_words(model, \"So, are we really at\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f2d7a-4743-466f-ba2e-fbff4419d981",
   "metadata": {},
   "source": [
    "#### This function takes a user input like “So, are we really at”, tokenizes and encodes the last few words, and passes them through the trained model to get output scores.\n",
    "\n",
    "#### These scores are then converted into probabilities using softmax, and the top k predictions (like the three most probable next words) are selected using torch.topk. The function then maps these indices back to actual words using idx2word, mimicking the behaviour of a real predictive keyboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
